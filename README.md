# ğŸ Snake AI - Q-Learning Training Dashboard

[![Python](https://img.shields.io/badge/Python-3.7+-blue.svg)](https://www.python.org/downloads/)
[![Pygame](https://img.shields.io/badge/Pygame-2.0+-green.svg)](https://www.pygame.org/)
[![License](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![AI](https://img.shields.io/badge/AI-Q--Learning-red.svg)](https://en.wikipedia.org/wiki/Q-learning)
[![Platform](https://img.shields.io/badge/Platform-Windows%20%7C%20macOS%20%7C%20Linux-lightgrey.svg)](https://github.com/)

A beautiful, modern implementation of the classic Snake game with an AI agent that learns to play using Q-Learning reinforcement learning. Features a professional training dashboard with real-time statistics and performance monitoring.

![Screenshot](screenshot.png)

## âœ¨ Features

- ğŸ¤– **Q-Learning AI Agent** - Learns to play Snake through trial and error
- ğŸ¨ **Beautiful Modern Interface** - Professional training dashboard with real-time stats
- ğŸ“Š **Real-time Performance Monitoring** - Track learning progress, exploration rate, and performance trends
- ğŸ’¾ **Model Persistence** - Save and load trained models
- ğŸ® **Interactive Controls** - Pause, reset, and save during training
- ğŸ“ˆ **Visual Learning Analytics** - Progress bars, trend indicators, and performance metrics

## ğŸ¯ What You'll See

The interface features:
- ğŸ® Large, clear game area with enhanced graphics
- ğŸ“‹ Real-time statistics panel showing score, episodes, and learning progress
- ğŸ” Exploration rate visualization with progress bars
- ğŸ“ˆ Performance trend indicators (Improving/Declining/Stable)
- â±ï¸ Training progress tracking
- ğŸŒ™ Modern dark theme with professional color scheme

## ğŸš€ Installation

### ğŸ“‹ Prerequisites

- ğŸ Python 3.7 or higher
- ğŸ“¦ pip package manager

### ğŸ“š Dependencies

Install the required libraries:

```bash
pip install pygame numpy
```

Or install them individually:

```bash
pip install pygame
pip install numpy
```

### ğŸ  Using Virtual Environment (Recommended)

```bash
# Create virtual environment
python -m venv snake_ai_env

# Activate it (Windows)
snake_ai_env\Scripts\activate

# Activate it (macOS/Linux)
source snake_ai_env/bin/activate

# Install dependencies
pip install pygame numpy
```

## ğŸ® Usage

### â–¶ï¸ Running the Training

```bash
python snake_ai.py
```

### ğŸ›ï¸ Controls

- **â¸ï¸ SPACE** - Pause/Resume training
- **ğŸ”„ R** - Reset training (clear all progress)
- **ğŸ’¾ S** - Save model manually
- **âŒ ESC** - Exit application

### ğŸŒŸ First Run

On first run, the AI will start learning from scratch. The model will be automatically saved as `snake_q_table.pkl` every 100 episodes and when you exit the program.

### ğŸ“‚ Loading Existing Model

If a saved model (`snake_q_table.pkl`) exists in the same directory, it will be automatically loaded when you start the program.

## ğŸ§  How It Works

### ğŸ¤– Q-Learning Algorithm

The AI uses Q-Learning, a model-free reinforcement learning algorithm that learns the quality of actions (Q-values) for different states:

1. **ğŸ¯ State Representation**: The game state includes:
   - âš ï¸ Danger detection (walls and snake body in each direction)
   - ğŸ Food direction relative to snake head
   - â¡ï¸ Current movement direction
   - ğŸ“Š This creates a 12-dimensional boolean state space

2. **ğŸ® Actions**: The snake can take 3 actions:
   - â¬†ï¸ Continue straight (0)
   - â†—ï¸ Turn right (1)
   - â†–ï¸ Turn left (2)

3. **ğŸ† Rewards**:
   - âœ… +10 for eating food
   - âŒ -10 for collision (game over)
   - â° -0.1 for each step (encourages efficiency)
   - ğŸ”„ -5 for taking too long without eating (prevents infinite loops)

4. **âš™ï¸ Learning Parameters**:
   - ğŸ“š Learning rate (Î±): 0.1
   - ğŸ”® Discount factor (Î³): 0.95
   - ğŸ² Exploration rate (Îµ): Starts at 1.0, decays to 0.01

### ğŸ”„ Training Process

1. **ğŸ² Exploration Phase**: Initially, the AI explores randomly (high Îµ value)
2. **ğŸ“š Learning Phase**: As training progresses, the AI gradually exploits learned knowledge
3. **ğŸ¯ Convergence**: Eventually, the AI develops optimal strategies for playing Snake

## ğŸ“ Project Structure

```
snake_ai/
â”‚
â”œâ”€â”€ ğŸ snake_ai.py           # Main game and AI implementation
â”œâ”€â”€ ğŸ’¾ snake_q_table.pkl     # Saved Q-table (created after training)
â”œâ”€â”€ ğŸ“¸ screenshot.png        # Project screenshot
â””â”€â”€ ğŸ“– README.md            # This file
```

## ğŸ—ï¸ Code Structure

### ğŸ§© Main Classes

- **`ğŸ® SnakeGame`** - Game logic and environment
- **`ğŸ¤– QLearningAgent`** - AI agent with Q-learning implementation
- **`ğŸ¨ GameRenderer`** - Modern UI and visualization

### ğŸ”§ Key Methods

- `get_state()` - Converts game situation to state representation
- `choose_action()` - Epsilon-greedy action selection
- `update()` - Q-value updates using Bellman equation
- `render()` - Beautiful dashboard rendering

## ğŸ“Š Performance Metrics

The dashboard displays several key metrics:

- **ğŸ† Score** - Current game score
- **ğŸ¬ Episode** - Number of games played
- **ğŸ§  States** - Number of unique states learned
- **ğŸ² Exploration Rate** - Current Îµ value (exploration vs exploitation)
- **ğŸ“ˆ Average Score** - Running average of last 100 games
- **ğŸ‘‘ Best Score** - Highest score achieved
- **ğŸ“Š Performance Trend** - Whether AI is improving, declining, or stable

## ğŸ’¡ Training Tips

1. **ğŸŒ± Initial Learning**: The first few hundred episodes will show poor performance as the AI explores
2. **ğŸ“ˆ Improvement Phase**: Around episodes 500-1000, you should see steady improvement
3. **ğŸ¯ Convergence**: After 1000+ episodes, performance should stabilize at a high level
4. **â³ Patience**: Q-Learning can take time to converge - let it run for several thousand episodes

## ğŸ›ï¸ Customization

### âš™ï¸ Adjusting Learning Parameters

In the `QLearningAgent` class constructor:

```python
agent = QLearningAgent(
    alpha=0.1,        # ğŸ“š Learning rate
    gamma=0.95,       # ğŸ”® Discount factor
    epsilon=1.0,      # ğŸ² Initial exploration rate
    epsilon_decay=0.995,  # ğŸ“‰ Exploration decay rate
    epsilon_min=0.01  # ğŸ¯ Minimum exploration rate
)
```

### ğŸ® Modifying Game Settings

In the constants section:

```python
GRID_SIZE = 40        # ğŸ“ Size of each grid cell
GRID_WIDTH = 10       # â†”ï¸ Number of cells horizontally
GRID_HEIGHT = 10      # â†•ï¸ Number of cells vertically
WINDOW_WIDTH = 1200   # ğŸ–¥ï¸ Window width
WINDOW_HEIGHT = 700   # ğŸ–¥ï¸ Window height
```

### ğŸ¨ Changing Visualization

The `GameRenderer` class contains all visual settings including colors, fonts, and layout parameters.

## ğŸ”§ Technical Details

- **ğŸ’» Language**: Python 3.7+
- **ğŸ® Graphics**: Pygame
- **ğŸ¤– AI Algorithm**: Q-Learning (Tabular)
- **ğŸ“Š State Space**: Discrete (2^12 possible states)
- **ğŸ¯ Action Space**: Discrete (3 actions)
- **ğŸ’¾ Model Persistence**: Pickle format

## ğŸ”§ Troubleshooting

### âš ï¸ Common Issues

1. **ğŸ“¦ Module not found**: Make sure pygame and numpy are installed
2. **ğŸŒ Slow performance**: Try reducing `render_every` parameter or window size
3. **ğŸ“‰ No improvement**: Increase training episodes or adjust learning parameters
4. **ğŸ’¾ Save/load errors**: Ensure write permissions in the project directory

### âš¡ Performance Optimization

- ğŸ–¥ï¸ Reduce `render_every` to update display less frequently
- ğŸ“± Decrease window size for faster rendering
- âš¡ Adjust `epsilon_decay` for faster convergence

## ğŸ“„ License

This project is open source and available under the MIT License.

## ğŸ™ Credits

- ğŸ Built with Python and Pygame
- ğŸ¤– Uses Q-Learning reinforcement learning algorithm
- ğŸ¨ Modern UI design inspired by professional ML dashboards

## ğŸ¤ Contributing

Feel free to fork this project and submit pull requests for improvements. Some areas where contributions would be welcome:

- ğŸ§  Additional AI algorithms
- âš¡ Performance optimizations
- ğŸ¨ UI/UX improvements
- ğŸ“Š Better state representations
- ğŸ“š Documentation improvements

---

**ğŸ‰ Happy Learning!** Watch your AI master the game of Snake through the power of reinforcement learning! ğŸğŸ¤–